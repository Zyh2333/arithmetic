# This would be a slice of C4
name: c4-subset
defaults:
  - sources:
      - c4

# Preprocessing
normalizer:
  force_lowercase: False
  strip_accents: False
  force_english_keyboard: False
tokenizer: SentencePieceBPE
vocab_size: 131072 # 2^17

# Dataset Formation
seq_length: 512
include_eot_token_in_corpus: True

max_entries_in_raw_dataset: 25e6 # Select only this many examples from the dataset # 20e6 are ok if all are chosen. Oversample if filtering
max_seq_in_tokenized_dataset: 85e6 # Select only this many tokenized sequences.
# max_seq_in_tokenized_dataset should be just slightly more than budget * 60 * 60 * expected tokens/sec for the single epoch of training

# Data Cleaning:
remove_trash: False
trash_cutoff: 0.25
deduplicate_entries: False
deduplication_threshold: 75

# Data Order:
ordering: randomized # for now

# Validation Split
validation_seqs: 4096 # how many sequences to reserve for validation
